{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "I1126 16:56:54.576250 140596968896256 file_utils.py:39] PyTorch version 1.3.1 available.\n",
      "I1126 16:56:54.645592 140596968896256 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pandas.io.json import json_normalize\n",
    "from operator import itemgetter\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from io import open\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import f1_score, mean_squared_error, matthews_corrcoef, confusion_matrix\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "import regex as re\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm_notebook, trange\n",
    "\n",
    "from transformers import WEIGHTS_NAME, BertForSequenceClassification, BertTokenizer\n",
    "from transformers import XLMForSequenceClassification, XLMTokenizer\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = re.compile(r\"([\\w0-9_\\.-]+)(@)([\\d\\w\\.-]+)(\\.)([\\w\\.]{2,6})\")\n",
    "URL = re.compile(r\"https?:\\/\\/(?!.*:\\/\\/)\\S+\")\n",
    "PHONE = re.compile(r\"(09|01[2|6|8|9])+([0-9]{8})\\b\")\n",
    "MENTION = re.compile(r\"@.+?:\")\n",
    "NUMBER = re.compile(r\"\\d+.?\\d*\")\n",
    "DATETIME = '\\d{1,2}\\s?[/-]\\s?\\d{1,2}\\s?[/-]\\s?\\d{4}'\n",
    "\n",
    "RE_HTML_TAG = re.compile(r'<[^>]+>')\n",
    "RE_CLEAR_1 = re.compile(\"[^_<>\\s\\p{Latin}]\")\n",
    "RE_CLEAR_2 = re.compile(\"__+\")\n",
    "RE_CLEAR_3 = re.compile(\"\\s+\")\n",
    "\n",
    "def replace_common_token(txt):\n",
    "    txt = re.sub(EMAIL, ' ', txt)\n",
    "    txt = re.sub(URL, ' ', txt)\n",
    "    txt = re.sub(MENTION, ' ', txt)\n",
    "    txt = re.sub(DATETIME, ' ', txt)\n",
    "    txt = re.sub(NUMBER, ' ', txt)\n",
    "    return txt\n",
    "\n",
    "def remove_emoji(txt):\n",
    "    txt = re.sub(':v', '', txt)\n",
    "    txt = re.sub(':D', '', txt)\n",
    "    txt = re.sub(':3', '', txt)\n",
    "    txt = re.sub(':\\(', '', txt)\n",
    "    txt = re.sub(':\\)', '', txt)\n",
    "    return txt\n",
    "\n",
    "def remove_html_tag(txt):\n",
    "    return re.sub(RE_HTML_TAG, ' ', txt)\n",
    "\n",
    "def remove_stop_words(txt):\n",
    "    tokenized_words = txt.split()\n",
    "    return \" \".join([word for word in tokenized_words if word not in stop_words])\n",
    "\n",
    "def preprocess(txt):\n",
    "#     txt = remove_html_tag(txt)\n",
    "#     txt = re.sub('&.{3,4};', ' ', txt)\n",
    "# #   txt = convertwindown1525toutf8(txt)\n",
    "# #   txt = replace_common_token(txt)\n",
    "#     txt = remove_emoji(txt)\n",
    "#     txt = re.sub(RE_CLEAR_1, ' ', txt)\n",
    "#     txt = re.sub(RE_CLEAR_2, ' ', txt)\n",
    "#     txt = re.sub(RE_CLEAR_3, ' ', txt)\n",
    "# #   txt = chuan_hoa_dau_cau_tieng_viet(txt)\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, id_a, text_a, id_b=None, text_b=None, label=None):\n",
    "        self.id_a = id_a\n",
    "        self.id_b = id_b\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    def get_train_examples(self, train_file):\n",
    "        examples = []\n",
    "        \n",
    "        train_df = pd.read_json(train_file, encoding= 'utf-8')\n",
    "        for index, row in train_df.iterrows():\n",
    "            quest = row['question']\n",
    "            quest = quest[:1].upper() + quest[1:]\n",
    "            title = row['title']\n",
    "            para = preprocess(row['text'])\n",
    "            label = str(int(bool(row['label'])))\n",
    "            \n",
    "            examples.append(InputExample(id_a=None, text_a=quest, id_b=None, text_b=title + \" . \" + para, label=label))\n",
    "        return examples\n",
    "        \n",
    "    def get_test_examples(self, test_file):\n",
    "        examples = []\n",
    "        \n",
    "        with open(test_file) as json_file:\n",
    "            test_json = json.load(json_file)\n",
    "        test_df = json_normalize(test_json, 'paragraphs', ['__id__', 'question', 'title'])\n",
    "        \n",
    "        for index, row in test_df.iterrows():\n",
    "            quest_id = row['__id__']\n",
    "            quest = row['question']\n",
    "            quest = quest[:1].upper() + quest[1:]\n",
    "            title = row['title']\n",
    "            para_id = row['id']\n",
    "            para = preprocess(row['text'])\n",
    "            \n",
    "            examples.append(InputExample(id_a=quest_id, text_a=quest, id_b=para_id, text_b=title + \" . \" + para, label=None))\n",
    "        return examples\n",
    "        \n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        return [\"0\", \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(example_row):\n",
    "    sequence_a_segment_id=0\n",
    "    sequence_b_segment_id=1\n",
    "    mask_padding_with_zero=True\n",
    "    \n",
    "    example, label_map, max_seq_length, tokenizer, cls_token_segment_id, pad_token_segment_id = example_row\n",
    "\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    \n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    # Create segment ids\n",
    "    tokens = tokens_a + [sep_token]\n",
    "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "    tokens += tokens_b + [sep_token]\n",
    "    segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
    "\n",
    "    tokens = [cls_token] + tokens\n",
    "    segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "    # Create input mask\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    if padding_length >= 0:\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
    "    else:\n",
    "        input_ids = input_ids[:max_seq_length]\n",
    "        input_mask = input_mask[:max_seq_length]\n",
    "        segment_ids = segment_ids[:max_seq_length]\n",
    "    \n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    \n",
    "    label_id = None\n",
    "    if example.label:\n",
    "        label_id = label_map[example.label]\n",
    "\n",
    "    return InputFeatures(input_ids=input_ids,\n",
    "                        input_mask=input_mask,\n",
    "                        segment_ids=segment_ids,\n",
    "                        label_id=label_id)\n",
    "    \n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer,\n",
    "                                 cls_token_segment_id=0, pad_token_segment_id=0):\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "    examples = [(example, label_map, max_seq_length, tokenizer, cls_token_segment_id, pad_token_segment_id) for example in examples]\n",
    "\n",
    "    process_count = cpu_count()\n",
    "    with Pool(process_count) as p:\n",
    "        features = list(tqdm(p.imap(convert_example_to_feature, examples, chunksize=100), total=len(examples)))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'model_type':  'bert',\n",
    "# 'model_name': 'bert-base-multilingual-cased',\n",
    "\n",
    "# 'model_type':  'xlm',\n",
    "# 'model_name': 'xlm-mlm-17-1280',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'data_dir': '/data/ai_challenge/vietnameseqa/data/',\n",
    "    'train_file': 'train.json',\n",
    "    'test_file': 'test.json',\n",
    "    'model_type': 'bert',\n",
    "    'model_name': 'bert-base-multilingual-cased',\n",
    "    'task_name': 'binary',\n",
    "    'output_dir': 'outputs/',\n",
    "    'cache_dir': 'cache/',\n",
    "    'do_eval': True,\n",
    "    'max_seq_length': 200,\n",
    "    'train_batch_size': 8,\n",
    "    'test_batch_size': 8,\n",
    "\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_train_epochs': 1,\n",
    "    'weight_decay': 1e-7,\n",
    "    'learning_rate': 1e-5,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_steps': 0,\n",
    "    'max_grad_norm': 1.0,\n",
    "\n",
    "    'logging_steps': 100,\n",
    "    'evaluate_during_training': False,\n",
    "    'create_checkpoint': False,\n",
    "    'save_steps': 2000,\n",
    "    'eval_all_checkpoints': True,\n",
    "    'reprocess_input_data': True,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 17:21:11.714929 140159970518784 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/thinhvd/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "I1124 17:21:12.716949 140159970518784 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/thinhvd/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.83b0fa3d7f1ac0e113ad300189a938c6f14d0588a4200f30eef109d0a047c484\n",
      "I1124 17:21:12.721658 140159970518784 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "I1124 17:21:13.538999 140159970518784 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin from cache at /home/thinhvd/.cache/torch/transformers/5b5b80054cd2c95a946a8e0ce0b93f56326dff9fbda6a6c3e02de3c91c918342.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
      "I1124 17:21:16.303232 140159970518784 modeling_utils.py:405] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I1124 17:21:16.303662 140159970518784 modeling_utils.py:408] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'bert': (BertForSequenceClassification, BertTokenizer),\n",
    "    'xlm': (XLMForSequenceClassification, XLMTokenizer),\n",
    "}\n",
    "\n",
    "model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(args['model_name'])\n",
    "model = model_class.from_pretrained(args['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(tokenizer, test=False):\n",
    "    processor = DataProcessor()\n",
    "    \n",
    "    logger.info(\"Creating features from dataset file at %s\", args['data_dir'])\n",
    "    label_list = processor.get_labels()\n",
    "    examples = processor.get_test_examples(args['data_dir'] + args['test_file']) if test else processor.get_train_examples(args['data_dir'] + args['train_file'])\n",
    "\n",
    "    features = convert_examples_to_features(examples, label_list, args['max_seq_length'], tokenizer,\n",
    "                                            cls_token_segment_id=0,\n",
    "                                            pad_token_segment_id=0)\n",
    "        \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    \n",
    "    # Output\n",
    "    if not test:\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    else: \n",
    "        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 17:21:21.556224 140159970518784 <ipython-input-10-24cce704e7f0>:4] Creating features from dataset file at /data/ai_challenge/vietnameseqa/data/\n",
      " 37%|███▋      | 6701/18108 [00:02<00:06, 1869.41it/s]W1124 17:21:25.979099 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      " 42%|████▏     | 7601/18108 [00:03<00:04, 2171.62it/s]W1124 17:21:26.337863 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      " 52%|█████▏    | 9501/18108 [00:04<00:03, 2339.40it/s]W1124 17:21:27.189528 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n",
      " 63%|██████▎   | 11401/18108 [00:04<00:03, 2199.88it/s]W1124 17:21:28.044523 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      " 65%|██████▌   | 11801/18108 [00:05<00:02, 2443.97it/s]W1124 17:21:28.110893 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      " 73%|███████▎  | 13190/18108 [00:05<00:01, 2473.36it/s]W1124 17:21:28.640950 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "W1124 17:21:28.645930 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      " 74%|███████▍  | 13466/18108 [00:05<00:01, 2375.74it/s]W1124 17:21:28.779641 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "W1124 17:21:28.849729 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      " 87%|████████▋ | 15701/18108 [00:06<00:01, 2055.33it/s]W1124 17:21:29.910437 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      " 88%|████████▊ | 15909/18108 [00:07<00:01, 2015.62it/s]W1124 17:21:30.034982 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      " 93%|█████████▎| 16801/18108 [00:07<00:00, 2147.75it/s]W1124 17:21:30.467797 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (730 > 512). Running this sequence through the model will result in indexing errors\n",
      "W1124 17:21:30.548603 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n",
      " 98%|█████████▊| 17734/18108 [00:07<00:00, 1872.22it/s]W1124 17:21:30.902716 140159970518784 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 18108/18108 [00:07<00:00, 2266.49it/s]\n",
      "I1124 17:21:31.092497 140159970518784 <ipython-input-11-7bf73b6382cb>:25] ***** Running training *****\n",
      "I1124 17:21:31.093137 140159970518784 <ipython-input-11-7bf73b6382cb>:26]   Num examples = 18108\n",
      "I1124 17:21:31.093618 140159970518784 <ipython-input-11-7bf73b6382cb>:27]   Num Epochs = 1\n",
      "I1124 17:21:31.094144 140159970518784 <ipython-input-11-7bf73b6382cb>:28]   Total train batch size  = 8\n",
      "I1124 17:21:31.094503 140159970518784 <ipython-input-11-7bf73b6382cb>:29]   Gradient Accumulation steps = 1\n",
      "I1124 17:21:31.094789 140159970518784 <ipython-input-11-7bf73b6382cb>:30]   Total optimization steps = 1811\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e69ea758ffb44a790008c8d6d1d6827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=1811, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.517139 - 0.750000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 17:25:46.344910 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.604013594686985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.942570 - 0.000000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 17:29:56.290971 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.5254137025773525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.476239 - 0.800000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 17:34:05.541409 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.5126111339777708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893886 - 0.500000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 17:38:28.508153 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.5063785469532013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.515242 - 0.666667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 17:42:51.530393 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.49125157579779627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.304526 - 0.800000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 17:47:18.098905 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.4772423630207777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.417090 - 0.500000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 17:51:37.003913 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.45955790795385837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.489325 - 0.500000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 17:56:06.656484 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.4478921489417553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.579581 - 0.500000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 18:00:25.952988 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.44140344828367234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558991 - 0.666667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 18:04:56.721226 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.441366666816175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.544780 - 0.400000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 18:09:27.605429 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.42437400184571744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.426828 - 0.857143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 18:13:57.329216 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.43303107157349585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.265039 - 0.857143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 18:18:26.310726 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.399442097954452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.632408 - 0.800000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 18:22:59.125565 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.40474018748849633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.222304 - 0.666667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 18:27:33.804156 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.3467920238897204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.111951 - 1.000000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 18:32:09.547373 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.38865942392498254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.425867 - 0.666667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 18:36:43.326843 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.39194029103964567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.762835 - 0.500000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 18:41:15.071382 140159970518784 <ipython-input-11-7bf73b6382cb>:74] loss = 0.3589174585416913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.382988 - 0.500000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 18:41:44.732513 140159970518784 <ipython-input-11-7bf73b6382cb>:77]  Train global_step = 1811, average loss = 0.44700860518659763, average f1 = 0.658465991316932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31cdeebc0d546f4a2ff755f263d85c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=453, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1124 18:46:13.374503 140159970518784 <ipython-input-11-7bf73b6382cb>:102]  Eval average loss = 0.38164129043224104, average f1 = 0.7372764786795047\n",
      "Epoch: 100%|██████████| 1/1 [1:24:42<00:00, 5082.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_and_cache_examples(tokenizer)\n",
    "if args['do_eval']:\n",
    "    train_dataset_size = len(train_dataset)\n",
    "    train_size = int(0.8 * train_dataset_size)\n",
    "    valid_size = train_dataset_size - train_size\n",
    "    \n",
    "    train_dataset_splitted, valid_dataset = random_split(train_dataset, (train_size, valid_size))\n",
    "    train_dataloader = DataLoader(train_dataset_splitted, batch_size=args['train_batch_size'], shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=args['train_batch_size'], shuffle=False)\n",
    "else:\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n",
    "\n",
    "t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "#     no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args['warmup_steps'], t_total=t_total)\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
    "logger.info(\"  Gradient Accumulation steps = %d\", args['gradient_accumulation_steps'])\n",
    "logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "model.zero_grad()\n",
    "train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "\n",
    "for epoch in train_iterator:\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    epoch_labels = []\n",
    "    epoch_preds = []\n",
    "    \n",
    "    epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        model.train()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                  'labels':         batch[3]}\n",
    "        labels = batch[3].detach().cpu().numpy()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        f1_scr = f1_score(labels, preds)\n",
    "        print(\"\\r%f - %f\" % (loss, f1_scr), end='')\n",
    "        \n",
    "        epoch_labels.extend(labels)\n",
    "        epoch_preds.extend(preds)\n",
    "        \n",
    "        if args['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "            scheduler.step()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            if args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0:\n",
    "                logger.info('loss = %s', (tr_loss - logging_loss)/args['logging_steps'])\n",
    "                logging_loss = tr_loss\n",
    "    \n",
    "    print(\" Train global_step = %f, average loss = %f, average f1 = %f\" % (global_step, tr_loss / global_step, f1_score(epoch_labels, epoch_preds)))\n",
    "    \n",
    "    \n",
    "    if args['do_eval']:\n",
    "        epoch_labels = []\n",
    "        epoch_preds = []\n",
    "        eval_loss = 0.0\n",
    "        \n",
    "        for step, batch in enumerate(tqdm_notebook(valid_dataloader, desc=\"Iteration\")):\n",
    "            model.eval()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "            \n",
    "            epoch_labels.extend(batch[3].detach().cpu().numpy())\n",
    "            epoch_preds.extend(preds)\n",
    "            \n",
    "        print(\" Eval average loss = %f, average f1 = %f\" % (eval_loss/len(valid_dataloader), f1_score(epoch_labels, epoch_preds)))\n",
    "        \n",
    "    # Save model\n",
    "    epoch_output_dir = os.path.join(args['data_dir'], args['output_dir'], \"epoch_\" + str(epoch))\n",
    "    if not os.path.exists(epoch_output_dir):\n",
    "        os.makedirs(epoch_output_dir)\n",
    "    model.save_pretrained(epoch_output_dir)\n",
    "    tokenizer.save_pretrained(epoch_output_dir)\n",
    "    torch.save(args, os.path.join(epoch_output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_submit_file(test_file, preds):\n",
    "    with open(test_file) as json_file:\n",
    "        test_json = json.load(json_file)\n",
    "    test_df = json_normalize(test_json, 'paragraphs', ['__id__', 'question', 'title'])\n",
    "    test_df['preds'] = preds\n",
    "    test_df = test_df.loc[test_df['preds'] == 1]\n",
    "    \n",
    "    submit_df = pd.DataFrame()\n",
    "    submit_df['test_id'] = test_df['__id__']\n",
    "    submit_df['answer'] = test_df['id']\n",
    "    submit_df.to_csv(args['data_dir'] + 'submits/sample_submission_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\") + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, tokenizer, prefix=\"\"):\n",
    "    results = {}\n",
    "    test_dataset = load_and_cache_examples(tokenizer, test=True)\n",
    "\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args['test_batch_size'])\n",
    "\n",
    "    logger.info(\"***** Running test {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(test_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args['test_batch_size'])\n",
    "    preds = None\n",
    "    model.eval()\n",
    "    for batch in tqdm_notebook(test_dataloader, desc=\"Testing\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs[0]\n",
    "            \n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    \n",
    "    export_submit_file(args['data_dir'] + args['test_file'], preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1128 21:55:03.004326 140596968896256 configuration_utils.py:148] loading configuration file /data/ai_challenge/vietnameseqa/data/outputs/epoch_5/config.json\n",
      "I1128 21:55:03.005117 140596968896256 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "I1128 21:55:03.005844 140596968896256 modeling_utils.py:334] loading weights file /data/ai_challenge/vietnameseqa/data/outputs/epoch_5/pytorch_model.bin\n",
      "I1128 21:55:05.609492 140596968896256 tokenization_utils.py:306] Model name '/data/ai_challenge/vietnameseqa/data/outputs/epoch_5' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/ai_challenge/vietnameseqa/data/outputs/epoch_5' is a path or url to a directory containing tokenizer files.\n",
      "I1128 21:55:05.609986 140596968896256 tokenization_utils.py:371] loading file /data/ai_challenge/vietnameseqa/data/outputs/epoch_5/vocab.txt\n",
      "I1128 21:55:05.610329 140596968896256 tokenization_utils.py:371] loading file /data/ai_challenge/vietnameseqa/data/outputs/epoch_5/added_tokens.json\n",
      "I1128 21:55:05.610659 140596968896256 tokenization_utils.py:371] loading file /data/ai_challenge/vietnameseqa/data/outputs/epoch_5/special_tokens_map.json\n",
      "I1128 21:55:05.610940 140596968896256 tokenization_utils.py:371] loading file /data/ai_challenge/vietnameseqa/data/outputs/epoch_5/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export test from pre_train model\n",
    "pretrain_model = os.path.join(args['data_dir'], args['output_dir'], \"epoch_5\")\n",
    "model = BertForSequenceClassification.from_pretrained(pretrain_model)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1128 21:55:18.758872 140596968896256 <ipython-input-7-24cce704e7f0>:4] Creating features from dataset file at /data/ai_challenge/vietnameseqa/data/\n",
      "100%|██████████| 3512/3512 [00:01<00:00, 2258.79it/s]\n",
      "I1128 21:55:21.258114 140596968896256 <ipython-input-9-1b93e49a2345>:8] ***** Running test  *****\n",
      "I1128 21:55:21.258772 140596968896256 <ipython-input-9-1b93e49a2345>:9]   Num examples = 3512\n",
      "I1128 21:55:21.259322 140596968896256 <ipython-input-9-1b93e49a2345>:10]   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120692b52e3d4d3bb4b97b689656bd01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Testing', max=439, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1128 22:01:37.812880 140596968896256 <ipython-input-20-8792cd99b14e>:2] Finished Export Submit File!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds = test(model, tokenizer)\n",
    "logger.info(\"Finished Export Submit File!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
