{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://dl.challenge.zalo.ai/ZAC2019_VietnameseWikiQA/ZAC2019_BuildingQAS_Slides.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models import KeyedVectors\n",
    "from underthesea import word_tokenize, sent_tokenize\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pandas.io.json import json_normalize\n",
    "from underthesea import word_tokenize, sent_tokenize\n",
    "from operator import itemgetter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/data/ai_challenge/vietnameseqa/data/train.json'\n",
    "test_file = '/data/ai_challenge/vietnameseqa/data/test.json'\n",
    "embedding_file = '/data/ai_challenge/vietnameseqa/embedding/wiki.vi.model.bin.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_max_len = 20\n",
    "para_max_len = 60\n",
    "title_max_len = 10\n",
    "max_words = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNTokenizer:\n",
    "    def __init__(self, num_words):\n",
    "        super(VNTokenizer, self).__init__()\n",
    "        self.num_words = num_words\n",
    "        self.word_counts = {}\n",
    "        self.word_index = {}\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        for text in texts:\n",
    "            tokens = word_tokenize(text)\n",
    "\n",
    "            for token in tokens:\n",
    "                if token not in self.word_counts:\n",
    "                    self.word_counts[token] = 1\n",
    "                else:\n",
    "                    self.word_counts[token] += 1\n",
    "            \n",
    "        for i, (token, count) in enumerate(sorted(self.word_counts.items(), key=itemgetter(1), reverse=True)[ : self.num_words]):\n",
    "            self.word_index[token] = i + 1\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            tokens = word_tokenize(text)\n",
    "            sequence = []\n",
    "            for token in tokens:\n",
    "                sequence.append(self.word_index[token])\n",
    "\n",
    "            sequences.append(sequence)\n",
    "\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_file, test_file):\n",
    "    train_df = pd.read_json(train_file, encoding= 'utf-8')\n",
    "\n",
    "    with open(test_file) as json_file:\n",
    "        test_json = json.load(json_file)\n",
    "    test_df = json_normalize(test_json, 'paragraphs', ['__id__', 'question', 'title'])\n",
    "    print('Train data:', train_df.shape)\n",
    "    print('Test data:', test_df.shape)\n",
    "    train_quests = train_df['question'].values\n",
    "    train_paras = train_df['text'].values\n",
    "    train_y = train_df['label'].astype(float).values\n",
    "\n",
    "    test_quest_ids = test_df['__id__'].values\n",
    "    test_quests = test_df['question'].values\n",
    "    test_para_ids = test_df['id'].values\n",
    "    test_paras = test_df['text'].values\n",
    "\n",
    "    tokenizer = VNTokenizer(max_words)\n",
    "    tokenizer.fit_on_texts(list(train_quests) + list(train_paras) + list(test_quests) + list(test_paras))\n",
    "    train_quests = tokenizer.texts_to_sequences(train_quests)\n",
    "    train_paras = tokenizer.texts_to_sequences(train_paras)\n",
    "    test_quests = tokenizer.texts_to_sequences(test_quests)\n",
    "    test_paras = tokenizer.texts_to_sequences(test_paras)\n",
    "\n",
    "    train_quests = pad_sequences(train_quests, maxlen=quest_max_len)\n",
    "    train_paras = pad_sequences(train_paras, maxlen=para_max_len)\n",
    "    test_quests = pad_sequences(test_quests, maxlen=quest_max_len)\n",
    "    test_paras = pad_sequences(test_paras, maxlen=para_max_len)\n",
    "\n",
    "    np.random.seed(2019)\n",
    "    trn_idx = np.random.permutation(len(train_quests))\n",
    "\n",
    "    train_quests = train_quests[trn_idx]\n",
    "    train_paras = train_paras[trn_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "\n",
    "    return train_quests, train_paras, train_y, test_quest_ids, test_quests, test_para_ids, test_paras, tokenizer.word_index\n",
    "\n",
    "\n",
    "def create_embed_matrix(word_index, embedding_file):\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True)\n",
    "    nb_words = min(max_words, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, word_vectors.vector_size))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i < nb_words:\n",
    "            word = word.replace(' ', '_')\n",
    "            if word in word_vectors:\n",
    "                embedding_matrix[i] = word_vectors[word]\n",
    "            elif word.lower() in word_vectors:\n",
    "                embedding_matrix[i] = word_vectors[word.lower()]\n",
    "            elif word.capitalize() in word_vectors:\n",
    "                embedding_matrix[i] = word_vectors[word.capitalize()]\n",
    "            elif word.upper() in word_vectors:\n",
    "                embedding_matrix[i] = word_vectors[word.upper()]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_threshold(y_true, preds):\n",
    "    best_thresh = 0.5\n",
    "    best_score = 0.0\n",
    "    for thresh in np.arange(0.3, 0.501, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        score = metrics.f1_score(y_true, (preds > thresh).astype(int))\n",
    "        if score > best_score:\n",
    "            best_thresh = thresh\n",
    "            best_score = score\n",
    "    return best_thresh, best_score\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim_1, hidden_dim_2, target_size):\n",
    "        super(Net, self).__init__()\n",
    "        input_dim = embedding_matrix.shape[1]\n",
    "\n",
    "        weights = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.lstm_ques = nn.LSTM(input_dim, hidden_dim_1, num_layers=1, batch_first=True)\n",
    "        self.lstm_para = nn.LSTM(input_dim, hidden_dim_1, num_layers=1, batch_first=True)\n",
    "\n",
    "        self.drop_ques = nn.Dropout(p=0.5)\n",
    "        self.drop_para = nn.Dropout(p=0.5)\n",
    "        self.drop_output = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.linear_1 = nn.Linear(hidden_dim_1 + hidden_dim_1, hidden_dim_2)\n",
    "        self.linear_2 = nn.Linear(hidden_dim_2, target_size)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        question, text = input_\n",
    "        ques_embed = self.embedding(question)\n",
    "        ques_output, ques_hidden = self.lstm_ques(ques_embed)\n",
    "\n",
    "        para_embed = self.embedding(text)\n",
    "        para_output, para_hidden = self.lstm_para(para_embed)\n",
    "\n",
    "        ques_hidden = torch.squeeze(ques_hidden[0], 0)\n",
    "        para_hidden = torch.squeeze(para_hidden[0], 0)\n",
    "\n",
    "        ques_hidden = self.drop_ques(ques_hidden)\n",
    "        para_hidden = self.drop_para(para_hidden)\n",
    "\n",
    "        compare_vec = torch.cat([ques_hidden, para_hidden], dim=1)\n",
    "        output_1 = self.linear_1(compare_vec)\n",
    "        output_1 = self.drop_output(output_1)\n",
    "        output_2 = self.linear_2(output_1)\n",
    "\n",
    "        return output_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoches = 5\n",
    "hidden_dim_1 = 200\n",
    "hidden_dim_2 = 200\n",
    "hidden_dim_3 = 400\n",
    "target_size = 1\n",
    "batch_size = 32\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Train data: (18108, 5)\n",
      "Test data: (2678, 5)\n",
      "Embedding matrix: (54325, 400)\n",
      "\n",
      "Train Fold 0\n",
      "Epoch: 0 - train_loss: 0.6284 - valid_loss: 0.6116\n",
      "Epoch: 1 - train_loss: 0.6095 - valid_loss: 0.6164\n",
      "Epoch: 2 - train_loss: 0.5708 - valid_loss: 0.5939\n",
      "Epoch: 3 - train_loss: 0.5160 - valid_loss: 0.6186\n",
      "Epoch: 4 - train_loss: 0.4428 - valid_loss: 0.6428\n",
      "Epoch: 5 - train_loss: 0.3709 - valid_loss: 0.6717\n",
      "Epoch: 6 - train_loss: 0.2953 - valid_loss: 0.8689\n",
      "Epoch: 7 - train_loss: 0.2407 - valid_loss: 0.9884\n",
      "Epoch: 8 - train_loss: 0.1953 - valid_loss: 1.1075\n",
      "Epoch: 9 - train_loss: 0.1604 - valid_loss: 1.0507\n",
      "Epoch: 10 - train_loss: 0.1315 - valid_loss: 1.3517\n",
      "Epoch: 11 - train_loss: 0.1174 - valid_loss: 1.3854\n",
      "Epoch: 12 - train_loss: 0.1070 - valid_loss: 1.5658\n",
      "Epoch: 13 - train_loss: 0.1008 - valid_loss: 1.4183\n",
      "Epoch: 14 - train_loss: 0.0822 - valid_loss: 1.6388\n",
      "Epoch: 15 - train_loss: 0.0914 - valid_loss: 1.5885\n",
      "Epoch: 16 - train_loss: 0.0851 - valid_loss: 1.8697\n",
      "Epoch: 17 - train_loss: 0.0773 - valid_loss: 1.6864\n",
      "Epoch: 18 - train_loss: 0.0623 - valid_loss: 1.8362\n",
      "Epoch: 19 - train_loss: 0.0944 - valid_loss: 1.4666\n",
      "\n",
      "Train Fold 1\n",
      "Epoch: 0 - train_loss: 0.6278 - valid_loss: 0.6056\n",
      "Epoch: 1 - train_loss: 0.6127 - valid_loss: 0.6183\n",
      "Epoch: 2 - train_loss: 0.5719 - valid_loss: 0.5909\n",
      "Epoch: 3 - train_loss: 0.5186 - valid_loss: 0.6106\n",
      "Epoch: 4 - train_loss: 0.4529 - valid_loss: 0.6600\n",
      "Epoch: 5 - train_loss: 0.3749 - valid_loss: 0.6617\n",
      "Epoch: 6 - train_loss: 0.3020 - valid_loss: 0.7328\n",
      "Epoch: 7 - train_loss: 0.2481 - valid_loss: 0.8726\n",
      "Epoch: 8 - train_loss: 0.2019 - valid_loss: 1.1196\n",
      "Epoch: 9 - train_loss: 0.1764 - valid_loss: 1.1325\n",
      "Epoch: 10 - train_loss: 0.1497 - valid_loss: 1.1782\n",
      "Epoch: 11 - train_loss: 0.1216 - valid_loss: 1.3935\n",
      "Epoch: 12 - train_loss: 0.1233 - valid_loss: 1.3776\n",
      "Epoch: 13 - train_loss: 0.0976 - valid_loss: 1.6606\n",
      "Epoch: 14 - train_loss: 0.0888 - valid_loss: 1.5138\n",
      "Epoch: 15 - train_loss: 0.0834 - valid_loss: 1.6509\n",
      "Epoch: 16 - train_loss: 0.0894 - valid_loss: 1.5192\n",
      "Epoch: 17 - train_loss: 0.0944 - valid_loss: 1.6821\n",
      "Epoch: 18 - train_loss: 0.0708 - valid_loss: 1.6717\n",
      "Epoch: 19 - train_loss: 0.0694 - valid_loss: 2.1112\n",
      "\n",
      "Train Fold 2\n",
      "Epoch: 0 - train_loss: 0.6262 - valid_loss: 0.6181\n",
      "Epoch: 1 - train_loss: 0.6118 - valid_loss: 0.6215\n",
      "Epoch: 2 - train_loss: 0.5847 - valid_loss: 0.5991\n",
      "Epoch: 3 - train_loss: 0.5410 - valid_loss: 0.6016\n",
      "Epoch: 4 - train_loss: 0.4839 - valid_loss: 0.6193\n",
      "Epoch: 5 - train_loss: 0.4148 - valid_loss: 0.6720\n",
      "Epoch: 6 - train_loss: 0.3626 - valid_loss: 0.7455\n",
      "Epoch: 7 - train_loss: 0.2874 - valid_loss: 0.8386\n",
      "Epoch: 8 - train_loss: 0.2383 - valid_loss: 0.9827\n",
      "Epoch: 9 - train_loss: 0.1905 - valid_loss: 1.0834\n",
      "Epoch: 10 - train_loss: 0.1665 - valid_loss: 1.1526\n",
      "Epoch: 11 - train_loss: 0.1400 - valid_loss: 1.3506\n",
      "Epoch: 12 - train_loss: 0.1282 - valid_loss: 1.3429\n",
      "Epoch: 13 - train_loss: 0.1183 - valid_loss: 1.4289\n",
      "Epoch: 14 - train_loss: 0.0991 - valid_loss: 1.5984\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d60fdcb4425d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mquests_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "train_quests, train_paras, train_y, test_quest_ids, test_quests, test_para_ids, test_paras, word_index = load_data(train_file, test_file)\n",
    "embedding_matrix = create_embed_matrix(word_index, embedding_file)\n",
    "print('Embedding matrix:', embedding_matrix.shape)\n",
    "\n",
    "test_quests = torch.tensor(test_quests, dtype=torch.long).to(device)\n",
    "test_paras = torch.tensor(test_paras, dtype=torch.long).to(device)\n",
    "test_dataset = TensorDataset(test_quests, test_paras)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=2019).split(train_quests, train_y))\n",
    "valid_preds = np.zeros(train_y.shape)\n",
    "test_preds = []\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    print('\\nTrain Fold {}'.format(idx))\n",
    "\n",
    "    train_model = Net(embedding_matrix, hidden_dim_1, hidden_dim_2, target_size)\n",
    "    train_model.to(device)\n",
    "\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    # optimizer = torch.optim.SGD(train_model.parameters(), lr=0.1)\n",
    "    optimizer = torch.optim.Adam(train_model.parameters())\n",
    "\n",
    "    quests_train = torch.tensor(train_quests[train_idx], dtype=torch.long).to(device)\n",
    "    quests_valid = torch.tensor(train_quests[valid_idx], dtype=torch.long).to(device)\n",
    "    paras_train = torch.tensor(train_paras[train_idx], dtype=torch.long).to(device)\n",
    "    paras_valid = torch.tensor(train_paras[valid_idx], dtype=torch.long).to(device)\n",
    "    y_train = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).to(device)\n",
    "    y_valid = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(quests_train, paras_train, y_train)\n",
    "    valid_dataset = TensorDataset(quests_valid, paras_valid, y_valid)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(n_epoches):\n",
    "        train_model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for i_batch, (quests_train, paras_train, y_train) in enumerate(train_loader):\n",
    "            y_pred = train_model((quests_train, paras_train))\n",
    "\n",
    "            loss = loss_fn(y_pred, y_train)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * quests_train.shape[0]\n",
    "\n",
    "        train_loss = train_loss / len(train_idx)\n",
    "\n",
    "        train_model.eval()\n",
    "        valid_loss = 0\n",
    "        fold_val_preds = np.zeros(len(valid_idx))\n",
    "        with torch.no_grad():\n",
    "            for i_batch, (quests_valid, paras_valid, y_valid) in enumerate(valid_loader):\n",
    "                y_pred = train_model((quests_valid, paras_valid)).detach()\n",
    "                valid_loss += loss_fn(y_pred, y_valid).item() * quests_valid.shape[0]\n",
    "                fold_val_preds[i_batch * batch_size : (i_batch + 1) * batch_size] = sigmoid(y_pred.cpu().numpy().squeeze())\n",
    "\n",
    "        valid_loss = valid_loss / len(valid_idx)\n",
    "        valid_preds[valid_idx] = fold_val_preds\n",
    "\n",
    "        print(\"Epoch: {} - train_loss: {:.4f} - valid_loss: {:.4f}\".format(epoch, train_loss, valid_loss))\n",
    "\n",
    "    train_model.eval()\n",
    "    fold_test_preds = np.zeros(len(test_quest_ids))\n",
    "    with torch.no_grad():\n",
    "        for i_batch, (test_quests, test_paras) in enumerate(test_loader):\n",
    "            y_pred = train_model((test_quests, test_paras)).detach()\n",
    "            fold_test_preds[i_batch * batch_size : (i_batch + 1) * batch_size] = sigmoid(y_pred.cpu().numpy().squeeze())\n",
    "\n",
    "    test_preds.append(fold_test_preds)\n",
    "\n",
    "# Output\n",
    "threshold, val_score = search_threshold(train_y, valid_preds)\n",
    "print('Valid score: {} - Best threshold: {}'.format(val_score, threshold))\n",
    "preds = np.mean(test_preds, axis=0)\n",
    "y_output = (preds > threshold).astype(int)\n",
    "\n",
    "submit_df = pd.DataFrame()\n",
    "submit_df['test_id'] = test_quest_ids[y_output==1]\n",
    "submit_df['answer'] = test_para_ids[y_output==1]\n",
    "submit_df.to_csv('../submits/sample_submission_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\") + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
