{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/kits/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "I1121 17:48:01.479136 139887018583808 file_utils.py:39] PyTorch version 1.3.1 available.\n",
      "I1121 17:48:01.516149 139887018583808 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models import KeyedVectors\n",
    "from underthesea import word_tokenize, sent_tokenize\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pandas.io.json import json_normalize\n",
    "from operator import itemgetter\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from io import open\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import f1_score, mean_squared_error, matthews_corrcoef, confusion_matrix\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm_notebook, trange\n",
    "\n",
    "from transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer)\n",
    "\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, id_a, text_a, id_b=None, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\"\"\"\n",
    "        self.id_a = id_a\n",
    "        self.id_b = id_b\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, train_file):\n",
    "        examples = []\n",
    "        \n",
    "        train_df = pd.read_json(train_file, encoding= 'utf-8')\n",
    "        for index, row in train_df.iterrows():\n",
    "            quest = row['question']\n",
    "            title = row['title']\n",
    "            para = row['text']\n",
    "            label = str(int(row['label']))\n",
    "            \n",
    "            examples.append(InputExample(id_a=None, text_a=quest, id_b=None, text_b=para, label=label))\n",
    "        return examples\n",
    "        \n",
    "    def get_test_examples(self, test_file):\n",
    "        examples = []\n",
    "        \n",
    "        with open(test_file) as json_file:\n",
    "            test_json = json.load(json_file)\n",
    "        test_df = json_normalize(test_json, 'paragraphs', ['__id__', 'question', 'title'])\n",
    "        \n",
    "        for index, row in test_df.iterrows():\n",
    "            quest_id = row['__id__']\n",
    "            quest = row['question']\n",
    "            title = row['title']\n",
    "            para_id = row['id']\n",
    "            para = row['text']\n",
    "            \n",
    "            examples.append(InputExample(id_a=quest_id, text_a=quest, id_b=para_id, text_b=para, label=None))\n",
    "        return examples\n",
    "        \n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        return [\"0\", \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(example_row, pad_token=0,\n",
    "                               sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
    "                               cls_token_segment_id=0, pad_token_segment_id=0,\n",
    "                               mask_padding_with_zero=True):\n",
    "    example, label_map, max_seq_length, tokenizer, cls_token, sep_token, cls_token_segment_id, pad_token_segment_id = example_row\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    # Create segment ids\n",
    "    tokens = tokens_a + [sep_token]\n",
    "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "    tokens += tokens_b + [sep_token]\n",
    "    segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
    "\n",
    "    tokens = [cls_token] + tokens\n",
    "    segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "    # Create input mask\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    if padding_length >= 0:\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
    "    else:\n",
    "        input_ids = input_ids[:max_seq_length]\n",
    "        input_mask = input_mask[:max_seq_length]\n",
    "        segment_ids = segment_ids[:max_seq_length]\n",
    "    \n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "#     print(tokens_a)\n",
    "#     print(tokens_b)\n",
    "#     print(input_ids)\n",
    "#     set_trace()\n",
    "    \n",
    "    label_id = None\n",
    "    if example.label:\n",
    "        label_id = label_map[example.label]\n",
    "\n",
    "    return InputFeatures(input_ids=input_ids,\n",
    "                        input_mask=input_mask,\n",
    "                        segment_ids=segment_ids,\n",
    "                        label_id=label_id)\n",
    "    \n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
    "                                 tokenizer,\n",
    "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
    "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
    "                                 cls_token_segment_id=0, pad_token_segment_id=0,\n",
    "                                 mask_padding_with_zero=True):\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "    examples = [(example, label_map, max_seq_length, tokenizer, cls_token, sep_token, cls_token_segment_id, pad_token_segment_id) for example in examples]\n",
    "\n",
    "    process_count = cpu_count() - 2\n",
    "    with Pool(process_count) as p:\n",
    "        features = list(tqdm(p.imap(convert_example_to_feature, examples, chunksize=100), total=len(examples)))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'data_dir': '/data/ai_challenge/vietnameseqa/data/',\n",
    "    'train_file': 'train.json',\n",
    "    'test_file': 'test.json',\n",
    "    'model_type':  'bert',\n",
    "    'model_name': 'bert-base-multilingual-cased',\n",
    "    'task_name': 'binary',\n",
    "    'output_dir': 'outputs/',\n",
    "    'cache_dir': 'cache/',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'max_seq_length': 128,\n",
    "    'train_batch_size': 8,\n",
    "    'eval_batch_size': 8,\n",
    "\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_train_epochs': 1,\n",
    "    'weight_decay': 0,\n",
    "    'learning_rate': 4e-5,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_steps': 0,\n",
    "    'max_grad_norm': 1.0,\n",
    "\n",
    "    'logging_steps': 50,\n",
    "    'evaluate_during_training': False,\n",
    "    'create_checkpoint': False,\n",
    "    'save_steps': 2000,\n",
    "    'eval_all_checkpoints': True,\n",
    "    'reprocess_input_data': True,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tokenizer, test=False):\n",
    "    processor = DataProcessor()\n",
    "    logger.info(\"Creating features from dataset file at %s\", args['data_dir'])\n",
    "    label_list = processor.get_labels()\n",
    "    \n",
    "    if not test:\n",
    "        examples = processor.get_train_examples(args['data_dir'] + args['train_file'])\n",
    "        features = convert_examples_to_features(examples, label_list, args['max_seq_length'], tokenizer,\n",
    "                                                cls_token=tokenizer.cls_token,\n",
    "                                                sep_token=tokenizer.sep_token,\n",
    "                                                cls_token_segment_id=0,\n",
    "                                                pad_token_segment_id=0)\n",
    "\n",
    "        return features\n",
    "    \n",
    "    else:\n",
    "        examples = processor.get_test_examples(args['data_dir'] + args['test_file'])\n",
    "        features = convert_examples_to_features(examples, label_list, args['max_seq_length'], tokenizer,\n",
    "                                                cls_token=tokenizer.cls_token,\n",
    "                                                sep_token=tokenizer.sep_token,\n",
    "                                                cls_token_segment_id=0,\n",
    "                                                pad_token_segment_id=0)\n",
    "        \n",
    "        test_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        test_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        test_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "        \n",
    "        test_dataset = TensorDataset(test_input_ids, test_input_mask, test_segment_ids)\n",
    "        test_sampler = SequentialSampler(test_dataset)\n",
    "        test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args['eval_batch_size'])\n",
    "        \n",
    "        return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_submit_file(test_file, all_preds):\n",
    "    preds = all_preds / 5\n",
    "    preds = [1 if i >= 0.5 else 0 for i in preds]    \n",
    "    \n",
    "    with open(test_file) as json_file:\n",
    "        test_json = json.load(json_file)\n",
    "    test_df = json_normalize(test_json, 'paragraphs', ['__id__', 'question', 'title'])\n",
    "    test_df['preds'] = preds\n",
    "    test_df = test_df.loc[test_df['preds'] == 1]\n",
    "    \n",
    "    submit_df = pd.DataFrame()\n",
    "    submit_df['test_id'] = test_df['__id__']\n",
    "    submit_df['answer'] = test_df['id']\n",
    "    submit_df.to_csv(args['data_dir'] + 'submits/sample_submission_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\") + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:19:42.016399 139982407472896 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /home/thinhvd/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
      "I1121 16:19:42.103125 139982407472896 <ipython-input-14-b16a02e78f4c>:3] Creating features from dataset file at /data/ai_challenge/vietnameseqa/data/\n",
      " 37%|███▋      | 6701/18108 [00:02<00:06, 1852.03it/s]W1121 16:19:46.414457 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      " 43%|████▎     | 7747/18108 [00:03<00:04, 2114.99it/s]W1121 16:19:46.804887 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      " 52%|█████▏    | 9347/18108 [00:04<00:04, 2075.69it/s]W1121 16:19:47.666061 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n",
      " 64%|██████▍   | 11554/18108 [00:05<00:02, 2239.05it/s]W1121 16:19:48.520889 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      " 66%|██████▌   | 11901/18108 [00:05<00:02, 2367.08it/s]W1121 16:19:48.590932 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      " 71%|███████   | 12801/18108 [00:05<00:02, 2245.28it/s]W1121 16:19:49.119681 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "W1121 16:19:49.124615 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      " 73%|███████▎  | 13201/18108 [00:05<00:02, 2142.02it/s]W1121 16:19:49.251844 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "W1121 16:19:49.302213 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      " 87%|████████▋ | 15710/18108 [00:07<00:01, 1847.33it/s]W1121 16:19:50.540330 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      " 88%|████████▊ | 15916/18108 [00:07<00:01, 1633.42it/s]W1121 16:19:50.666362 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      " 95%|█████████▍| 17122/18108 [00:07<00:00, 2093.66it/s]W1121 16:19:51.106426 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (730 > 512). Running this sequence through the model will result in indexing errors\n",
      "W1121 16:19:51.219562 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n",
      " 98%|█████████▊| 17801/18108 [00:07<00:00, 2245.46it/s]W1121 16:19:51.465212 139982407472896 tokenization_utils.py:677] Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 18108/18108 [00:08<00:00, 2247.64it/s]\n",
      "I1121 16:19:51.615054 139982407472896 <ipython-input-14-b16a02e78f4c>:3] Creating features from dataset file at /data/ai_challenge/vietnameseqa/data/\n",
      "100%|██████████| 2678/2678 [00:01<00:00, 2131.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:19:54.307997 139982407472896 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/thinhvd/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.83b0fa3d7f1ac0e113ad300189a938c6f14d0588a4200f30eef109d0a047c484\n",
      "I1121 16:19:54.311425 139982407472896 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "I1121 16:19:55.163399 139982407472896 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin from cache at /home/thinhvd/.cache/torch/transformers/5b5b80054cd2c95a946a8e0ce0b93f56326dff9fbda6a6c3e02de3c91c918342.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
      "I1121 16:19:57.915306 139982407472896 modeling_utils.py:405] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I1121 16:19:57.915863 139982407472896 modeling_utils.py:408] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "I1121 16:19:57.919345 139982407472896 <ipython-input-18-aa23d39ff0e9>:48] ***** Running kfolds 0 *****\n",
      "I1121 16:19:57.919922 139982407472896 <ipython-input-18-aa23d39ff0e9>:49]   Num train examples = 14486\n",
      "I1121 16:19:57.920186 139982407472896 <ipython-input-18-aa23d39ff0e9>:50]   Num valid examples = 3622\n",
      "I1121 16:19:57.920419 139982407472896 <ipython-input-18-aa23d39ff0e9>:51]   Num Epochs = 1\n",
      "I1121 16:19:57.920695 139982407472896 <ipython-input-18-aa23d39ff0e9>:52]   Total train batch size  = 8\n",
      "I1121 16:19:57.920984 139982407472896 <ipython-input-18-aa23d39ff0e9>:53]   Gradient Accumulation steps = 1\n",
      "I1121 16:19:57.921243 139982407472896 <ipython-input-18-aa23d39ff0e9>:54]   Total optimization steps = 1811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77784710deb94bfca3ee75b64b9a0e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=1811, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.430478"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:22:07.786190 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.579167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.571300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:24:14.475793 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.519407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.454158"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:26:22.964429 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.529891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.426434"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:28:31.625786 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.510113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995664"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:30:40.038077 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.583542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.539857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:32:48.507176 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.552434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.412913"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:34:53.274606 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.590257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.014968"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:37:00.330760 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.535445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.643940"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:39:10.737546 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.521648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625057"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:41:24.587378 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.536389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.714369"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:43:37.554720 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.492245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.795915"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:45:43.054695 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.497754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.268978"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:47:51.373232 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.488951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.876515"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:49:58.435474 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.473299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.295239"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:52:04.618452 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.481282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.075687"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:54:11.157785 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.496460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.185331"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:56:16.945099 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.511279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.494734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 16:58:22.478798 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.469914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.281266"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:00:30.620354 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.503204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.428183"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:02:41.236700 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.417011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.391885"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:04:48.168551 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.409459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.534073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:06:54.126525 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.436620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.676727"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:09:01.072154 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.393424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.120374"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:11:06.648174 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.485414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.208022"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:13:11.437283 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.422013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.721711"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:15:15.455099 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.387030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.612316"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:17:19.464758 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.411640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.617452"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:19:23.278908 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.381125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.408824"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:21:26.943753 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.417023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.153973"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:23:30.743607 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.416425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.461007"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:25:34.556547 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.419191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.457242"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:27:40.095969 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.394976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.160324"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:29:46.104550 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.421874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.086642"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:31:53.570724 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.329842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.056057"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:33:58.559988 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.390656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.217096"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:36:04.545773 139982407472896 <ipython-input-18-aa23d39ff0e9>:89]   Loss = 0.357226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.278560\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17172ae51d324d7e8391eeb987c7bd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=453, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:40:41.077284 139982407472896 <ipython-input-18-aa23d39ff0e9>:110] ***** Running test kfolds 0 *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0 - train_loss: 0.46567 - valid_loss: 0.37737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b116a40ba3e84a1bbcd6d4f219ec9b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=335, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1121 17:43:53.992027 139982407472896 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /home/thinhvd/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.83b0fa3d7f1ac0e113ad300189a938c6f14d0588a4200f30eef109d0a047c484\n",
      "I1121 17:43:53.994375 139982407472896 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "I1121 17:43:54.877073 139982407472896 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin from cache at /home/thinhvd/.cache/torch/transformers/5b5b80054cd2c95a946a8e0ce0b93f56326dff9fbda6a6c3e02de3c91c918342.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
      "I1121 17:43:58.148908 139982407472896 modeling_utils.py:405] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I1121 17:43:58.149452 139982407472896 modeling_utils.py:408] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "I1121 17:43:58.153743 139982407472896 <ipython-input-18-aa23d39ff0e9>:48] ***** Running kfolds 1 *****\n",
      "I1121 17:43:58.154034 139982407472896 <ipython-input-18-aa23d39ff0e9>:49]   Num train examples = 14486\n",
      "I1121 17:43:58.154298 139982407472896 <ipython-input-18-aa23d39ff0e9>:50]   Num valid examples = 3622\n",
      "I1121 17:43:58.154916 139982407472896 <ipython-input-18-aa23d39ff0e9>:51]   Num Epochs = 1\n",
      "I1121 17:43:58.155247 139982407472896 <ipython-input-18-aa23d39ff0e9>:52]   Total train batch size  = 8\n",
      "I1121 17:43:58.155499 139982407472896 <ipython-input-18-aa23d39ff0e9>:53]   Gradient Accumulation steps = 1\n",
      "I1121 17:43:58.155755 139982407472896 <ipython-input-18-aa23d39ff0e9>:54]   Total optimization steps = 1811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e27f30839d48d6b943252f32b61ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=1811, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.688691"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(args['model_name'])\n",
    "\n",
    "features = np.array(load_data(tokenizer))\n",
    "labels = [f.label_id for f in features]\n",
    "\n",
    "all_preds = None\n",
    "test_dataloader = load_data(tokenizer, test=True)\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=2019).split(features, labels))\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    seed_everything(idx)\n",
    "    print('\\nTrain Fold {}'.format(idx))\n",
    "#     set_trace()\n",
    "    train_features = features[train_idx]\n",
    "    valid_features = features[valid_idx]\n",
    "\n",
    "    train_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "    train_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "    train_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "    train_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "    \n",
    "    valid_input_ids = torch.tensor([f.input_ids for f in valid_features], dtype=torch.long)\n",
    "    valid_input_mask = torch.tensor([f.input_mask for f in valid_features], dtype=torch.long)\n",
    "    valid_segment_ids = torch.tensor([f.segment_ids for f in valid_features], dtype=torch.long)\n",
    "    valid_label_ids = torch.tensor([f.label_id for f in valid_features], dtype=torch.long)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_input_ids, train_input_mask, train_segment_ids, train_label_ids)\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n",
    "    \n",
    "    valid_dataset = TensorDataset(valid_input_ids, valid_input_mask, valid_segment_ids, valid_label_ids)\n",
    "    valid_sampler = RandomSampler(valid_dataset)\n",
    "    valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=args['train_batch_size'])\n",
    "    \n",
    "    if idx > 0:\n",
    "        model = model.cpu()\n",
    "        del model\n",
    "    gc.collect()\n",
    "    model = BertForSequenceClassification.from_pretrained(args['model_name'])\n",
    "    model.to(device)\n",
    "    \n",
    "    t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "#     no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    \n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
    "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args['warmup_steps'], t_total=t_total)\n",
    "   \n",
    "    logger.info(\"***** Running kfolds %d *****\", idx)\n",
    "    logger.info(\"  Num train examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num valid examples = %d\", len(valid_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args['gradient_accumulation_steps'])\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    \n",
    "    for epoch in range(int(args['num_train_epochs'])):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2],\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            print(\"\\r%f\" % loss, end='')\n",
    "\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0:\n",
    "                    logger.info(\"  Loss = %f\", (tr_loss - logging_loss)/args['logging_steps'])\n",
    "                    logging_loss = tr_loss\n",
    "                    \n",
    "        # Run validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_iterator = tqdm_notebook(valid_dataloader, desc=\"Iteration\")\n",
    "        for step, batch in enumerate(valid_iterator):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = {'input_ids':      batch[0],\n",
    "                          'attention_mask': batch[1],\n",
    "                          'token_type_ids': batch[2],\n",
    "                          'labels':         batch[3]}\n",
    "                outputs = model(**inputs)\n",
    "                valid_loss += outputs[0]\n",
    "                \n",
    "        print(\"Epoch: {} - train_loss: {:.5f} - valid_loss: {:.5f}\".format(epoch, epoch_loss/len(train_dataloader), valid_loss/len(valid_dataloader)))\n",
    "        \n",
    "    # Run test when finish a kfold\n",
    "    logger.info(\"***** Running test kfolds {} *****\".format(idx))\n",
    "    preds = None\n",
    "    \n",
    "    model.eval()\n",
    "    for batch in tqdm_notebook(test_dataloader, desc=\"Evaluating\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2]}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs[0]\n",
    "            \n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    if all_preds is None:\n",
    "        all_preds = preds\n",
    "    else:\n",
    "        all_preds += preds\n",
    "        \n",
    "# Export test result\n",
    "export_submit_file(args['data_dir'] + args['test_file'], all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
